{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Own\n",
    "from waste_dataset import WasteDatasetImages\n",
    "from classifier.simpleCNN import SimpleClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/dtu/datasets1/02514/data_wastedetection\"\n",
    "output_dir = \"output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataloader):    # save image with bboxes\n",
    "    num_images_to_process = 10\n",
    "    for idx, (image, bboxes) in enumerate(dataloader):\n",
    "        if idx >= num_images_to_process:\n",
    "            break\n",
    "\n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox = [item.item() for item in bbox]\n",
    "            x, y, width, height = bbox\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"image_{idx}.jpg\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Processed image {idx + 1}/{len(dataloader)}\")\n",
    "\n",
    "    print(\"Images with bounding boxes saved.\")\n",
    "\n",
    "def draw_bboxes(image, bboxes, labels, index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x, y, width, height = bbox\n",
    "        rect = patches.Rectangle((x,y), width, height, linewidth=1, edgecolor='r' if label == 1 else 'b', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    if not os.path.exists(\"inference\"):\n",
    "        os.makedirs(\"inference\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'inference/(bbox_visualization_on_test_images_{index}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bboxes(image, proposals, ground_truth_bboxes, iou_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox in ground_truth_bboxes:\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "        rect = patches.Rectangle((bbox_x, bbox_y), bbox_w, bbox_h,\n",
    "                                 linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    for proposal, iou_score in zip(proposals, iou_scores):\n",
    "        proposal_x, proposal_y, proposal_w, proposal_h = proposal\n",
    "\n",
    "        if iou_score > 0.2:\n",
    "            rect = patches.Rectangle((proposal_x, proposal_y), proposal_w, proposal_h,\n",
    "                                 linewidth=1, edgecolor='b', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(proposal_x, proposal_y, f\"IOU: {float(iou_score):.2f}\", color='r')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig('bbox_visualization.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_proposals_and_labels(dataloader, ss, num_images_to_process, max_proposals_per_image):\n",
    "    data_list           = []\n",
    "    proposals_box_list  = []\n",
    "    resized_images      = []\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for image, bboxes in dataloader:\n",
    "        print(f\"Processing image {counter + 1}/{num_images_to_process}\")\n",
    "        if counter >= num_images_to_process:\n",
    "            break\n",
    "        \n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # selective search\n",
    "        ss.setBaseImage(img)\n",
    "        # ss.switchToSelectiveSearchQuality() #Maybe try with fast\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "        rects = ss.process()\n",
    "\n",
    "        proposal_bbox   = []\n",
    "        proposal_image  = []\n",
    "\n",
    "        for i, rect in enumerate(rects): # loop for each region in image\n",
    "            if i >= max_proposals_per_image:\n",
    "                break\n",
    "            x, y, width, height = rect \n",
    "            bbox = [int(x), int(y), int(width), int(height)]  \n",
    "            proposal_bbox.append(bbox)\n",
    "            \n",
    "            proposal_img = img[y:y+height, x:x+width]\n",
    "            proposal_img = cv2.resize(proposal_img, (224, 224))\n",
    "            # proposal_features = extract_vgg16_features(proposal_image)\n",
    "            proposal_image.append((proposal_bbox, np.ravel(proposal_img)))\n",
    "\n",
    "        proposals_box_list.append(proposal_bbox)\n",
    "        resized_images.append(proposal_image)\n",
    "        # features_list.extend(features)\n",
    "\n",
    "        labels = assign_labels(proposal_bbox, bboxes, image)\n",
    "        data_list.extend(list(zip(proposal_bbox, labels)))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return data_list, proposals_box_list, resized_images\n",
    "\n",
    "def extract_color_histogram(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist\n",
    "\n",
    "# def extract_vgg16_features(image):\n",
    "#     image = cv2.resize(image, (224, 224))\n",
    "#     image = torch.tensor(image).permute(2, 0, 1).float().to(device)\n",
    "#     image = image.unsqueeze(0)\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         features = vgg16(image)\n",
    "#\n",
    "#     features = features.cpu().numpy() \n",
    "#     features = features.flatten() \n",
    "#\n",
    "#     return features\n",
    "\n",
    "\n",
    "def assign_labels(proposals, bboxes, image, iou_threshold=0.5):\n",
    "    labels = []\n",
    "    iou_scores = []\n",
    "\n",
    "    image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "    for proposal in proposals:\n",
    "        proposal_x1, proposal_y1, proposal_w, proposal_h = proposal\n",
    "        proposal_area = proposal_w * proposal_h\n",
    "\n",
    "        max_iou = 0.0  # Initialize max IOU score for each proposal\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox_x1, bbox_y1, bbox_w, bbox_h = bbox\n",
    "\n",
    "            intersection_x1 = max(proposal_x1, bbox_x1)\n",
    "            intersection_y1 = max(proposal_y1, bbox_y1)\n",
    "            intersection_x2 = min(proposal_x1 + proposal_w, bbox_x1 + bbox_w)\n",
    "            intersection_y2 = min(proposal_y1 + proposal_h, bbox_y1 + bbox_h)\n",
    "\n",
    "            intersection_area = max(0, intersection_x2 - intersection_x1) * max(0, intersection_y2 - intersection_y1)\n",
    "            bbox_area = bbox_w * bbox_h\n",
    "            union_area = proposal_area + bbox_area - intersection_area\n",
    "\n",
    "            iou = intersection_area / union_area\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "\n",
    "        if max_iou >= iou_threshold:\n",
    "            labels.append(1)  # Object proposal is a positive example\n",
    "        else:\n",
    "            labels.append(0)  # Object proposal is a negative example\n",
    "\n",
    "    num_ones = labels.count(1)\n",
    "    print(f\"Number of predictions: {num_ones}\")\n",
    "\n",
    "    # To visualize the bounding boxes and IOU scores, uncomment the following line\n",
    "    #visualize_bboxes(image, proposals, bboxes, iou_scores)\n",
    "    return labels    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WasteDatasetImages.__init__() got an unexpected keyword argument 'resize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ctrain \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m29\u001b[39m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[39m=\u001b[39m WasteDatasetImages(transform\u001b[39m=\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mToTensor(), resize\u001b[39m=\u001b[39;49m(\u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n\u001b[1;32m      8\u001b[0m \u001b[39m# Split the dataset into train and test sets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(dataset))\n",
      "\u001b[0;31mTypeError\u001b[0m: WasteDatasetImages.__init__() got an unexpected keyword argument 'resize'"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "ctest = 0\n",
    "ctrain = 0\n",
    "num_classes = 29\n",
    "\n",
    "dataset = WasteDatasetImages(transform=transforms.ToTensor(), resize=(224, 224))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "num_images_to_process_train = len(train_dataset) #Amount of train images to process\n",
    "num_images_to_process_test = len(test_dataset) #Amount of test images to process\n",
    "max_proposals_per_image = 1000 # Selective search will generate max 1000 proposals per image\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Run selective search \n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "print(\"Generating proposals and labels for train set\")\n",
    "train_proposals, train_data, train_image = generate_proposals_and_labels(train_dataloader, ss, num_images_to_process_train, max_proposals_per_image)\n",
    "\n",
    "raise Exception()\n",
    "# Extract the features and their corresponding bounding boxes\n",
    "train_features_flat, train_boxes = zip(*[(feature, bbox) for bbox, feature in train_features])\n",
    "train_features_flat = list(train_features_flat)\n",
    "train_boxes = list(train_boxes)\n",
    "\n",
    "train_labels = [label for _, label in train_data]\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading Simple classifier model\")\n",
    "model = SimpleClassifier(num_classes).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# svm = LinearSVC(C=0.1, class_weight='balanced') #balanced means that we give more weight to the minority class\n",
    "\n",
    "#Trains on the VGG16 features and the corresponding labels(0 or 1) Should be chnaged to multicass\n",
    "# svm.fit(train_features_flat, train_labels) \n",
    "\n",
    "print(\"Training done\")\n",
    "print(\"Saving model\")\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# dump(svm, 'models/svm_model.joblib')\n",
    "\n",
    "\n",
    "print(\"Generating proposals and labels for test set\")\n",
    "test_proposals, test_data, test_image = generate_proposals_and_labels(test_dataloader, ss, num_images_to_process_test, max_proposals_per_image)\n",
    "\n",
    "# Extract the features and their corresponding bounding boxes\n",
    "test_features_flat, test_boxes = zip(*[(feature, bbox) for bbox, feature in test_features])\n",
    "test_features_flat = list(test_features_flat)\n",
    "test_boxes = list(test_boxes)\n",
    "\n",
    "test_labels = [label for _, label in test_data]\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# svm = load('models/svm_model.joblib')\n",
    "\n",
    "# Make predictions on the test data\n",
    "# predictions = svm.predict(test_features_flat)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "number_of_images = 20 #Test images to visualize\n",
    "\n",
    "for i, img in enumerate(test_images[:number_of_images]):\n",
    "    # features_per_image = test_features_per_image[i]\n",
    "\n",
    "    predictions_by_image = []\n",
    "    bbox_list_by_image = []\n",
    "\n",
    "    # for feature in features_per_image:\n",
    "    #     bbox, feature_hist = feature \n",
    "    #     prediction = svm.predict([feature_hist])[0] \n",
    "    #     predictions_by_image.append(prediction)\n",
    "    #     bbox_list_by_image.append(bbox)\n",
    "\n",
    "    object_indices = [j for j, pred in enumerate(predictions_by_image) if pred == 1]            \n",
    "    object_bboxes = [bbox_list_by_image[index] for index in object_indices]\n",
    "    draw_bboxes(img, object_bboxes, [1] * len(object_bboxes), i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
