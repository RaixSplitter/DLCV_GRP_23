{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "# from torchvision.transforms import functional as F\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Own\n",
    "from waste_dataset import WasteDatasetImages\n",
    "from classifier.simpleCNN import SimpleClassifier\n",
    "from classifier.train_classifier import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/dtu/datasets1/02514/data_wastedetection\"\n",
    "output_dir = \"output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataloader):    # save image with bboxes\n",
    "    num_images_to_process = 10\n",
    "    for idx, (image, bboxes) in enumerate(dataloader):\n",
    "        if idx >= num_images_to_process:\n",
    "            break\n",
    "\n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox = [item.item() for item in bbox]\n",
    "            x, y, width, height = bbox\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"image_{idx}.jpg\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Processed image {idx + 1}/{len(dataloader)}\")\n",
    "\n",
    "    print(\"Images with bounding boxes saved.\")\n",
    "\n",
    "def plot_images_jupyter(images_og, train_proposals, train_label, num_images = 8):\n",
    "    num_images = num_images\n",
    "    for idx, (image, bboxes, labels) in enumerate(zip(images_og, train_proposals, train_label)):\n",
    "        if idx >= num_images:\n",
    "            break\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        has_label = set()\n",
    "        for bbox,lab in zip(bboxes, labels):\n",
    "            # bbox = [item.item() for item in bbox]\n",
    "            x, y, width, height = bbox\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r' if lab == 0 else \"blue\", facecolor='none', label = lab if lab not in has_label else \"\")\n",
    "            ax.add_patch(rect)\n",
    "            has_label.add(lab)\n",
    "        ax.legend()\n",
    "\n",
    "def draw_bboxes(image, bboxes, labels, index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x, y, width, height = bbox\n",
    "        rect = patches.Rectangle((x,y), width, height, linewidth=1, edgecolor='r' if label == 1 else 'b', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    if not os.path.exists(\"inference\"):\n",
    "        os.makedirs(\"inference\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'inference/(bbox_visualization_on_test_images_{index}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bboxes(image, proposals, ground_truth_bboxes, iou_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox in ground_truth_bboxes:\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "        rect = patches.Rectangle((bbox_x, bbox_y), bbox_w, bbox_h,\n",
    "                                 linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    for proposal, iou_score in zip(proposals, iou_scores):\n",
    "        proposal_x, proposal_y, proposal_w, proposal_h = proposal\n",
    "\n",
    "        if iou_score > 0.2:\n",
    "            rect = patches.Rectangle((proposal_x, proposal_y), proposal_w, proposal_h,\n",
    "                                 linewidth=1, edgecolor='b', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(proposal_x, proposal_y, f\"IOU: {float(iou_score):.2f}\", color='r')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig('bbox_visualization.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "def extract_color_histogram(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def generate_proposals_and_labels(dataloader, ss, num_images_to_process, max_proposals_per_image, img_shape = (224,224)):\n",
    "    data_list           = []\n",
    "    images              = []\n",
    "    resized_images      = []\n",
    "    proposals_box_list  = []\n",
    "    proposals_labels    = []\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for image, gt_bboxes, gt_label in tqdm(dataloader, desc=f\"Processing images\",total=num_images_to_process, leave=True):\n",
    "        if counter >= num_images_to_process:\n",
    "            break\n",
    "\n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "        images.append(img)\n",
    "        \n",
    "        # selective search\n",
    "        ss.setBaseImage(img)\n",
    "        ss.switchToSelectiveSearchQuality() #Maybe try with fast\n",
    "        # ss.switchToSelectiveSearchFast()\n",
    "        rects = ss.process()\n",
    "\n",
    "        proposal_bbox   = []\n",
    "        proposal_image  = []\n",
    "\n",
    "        for i, rect in enumerate(rects): # loop for each region in image\n",
    "            if i >= max_proposals_per_image: \n",
    "                break\n",
    "            x, y, width, height = rect\n",
    "            bbox = [int(x), int(y), int(width), int(height)]\n",
    "            proposal_bbox.append(bbox)\n",
    "            \n",
    "            proposal_img = img[y:y+height, x:x+width]\n",
    "            proposal_img = cv2.resize(proposal_img, img_shape)\n",
    "            proposal_img = np.transpose(proposal_img, [2,0,1]) # Rearrange axis\n",
    "            proposal_image.append(proposal_img)\n",
    "\n",
    "        labels = assign_labels(proposal_bbox, gt_bboxes, gt_label, image)\n",
    "        \n",
    "        # Remove gray-zones (inbetween threshold)\n",
    "        for i, label in reversed(list(enumerate(labels))):\n",
    "            if label == -1:\n",
    "                del labels[i]\n",
    "                del proposal_bbox[i]\n",
    "                del proposal_image[i]\n",
    "        \n",
    "        # Limit background patches, ratio 8 to 1 non background \n",
    "        num_non_bg = len(labels) - labels.count(0)\n",
    "        num_bg     = labels.count(0)\n",
    "        limit_bg   = num_non_bg*8\n",
    "        if limit_bg < num_bg: # Too many backgrounds\n",
    "            transfer_labels = []\n",
    "            transfer_bboxes = []\n",
    "            transfer_images = []\n",
    "            for idx in range(len(labels)):\n",
    "                if labels[idx] == 0 and limit_bg == 0:  \n",
    "                    continue\n",
    "                elif labels[idx] == 0:                  \n",
    "                    limit_bg -= 1\n",
    "                transfer_labels.append(        labels[idx])\n",
    "                transfer_bboxes.append( proposal_bbox[idx])\n",
    "                transfer_images.append(proposal_image[idx])\n",
    "            labels          = transfer_labels\n",
    "            proposal_bbox   = transfer_bboxes\n",
    "            proposal_image  = transfer_images\n",
    "        \n",
    "        # # Adding ground truth     # Err0r: gt-bbox may be too small due to transforming, resulting in proposal_img = None \n",
    "        # for idx in range(len(gt_label)):\n",
    "        #     labels.append(int(gt_label[idx]))\n",
    "        #     x, y, width, height = int(gt_bboxes[idx][0]), int(gt_bboxes[idx][1]), int(gt_bboxes[idx][2]), int(gt_bboxes[idx][3]) \n",
    "        #     bbox = [x, y, width, height]\n",
    "        #     proposal_bbox.append(bbox)\n",
    "        #     proposal_img = img[y:y+height, x:x+width]\n",
    "        #     proposal_img = cv2.resize(proposal_img, img_shape)\n",
    "        #     proposal_img = np.transpose(proposal_img, [2,0,1])\n",
    "        #     proposal_image.append(proposal_img)   \n",
    "        \n",
    "        # Extend\n",
    "        proposals_box_list.extend(proposal_bbox)\n",
    "        resized_images.extend(proposal_image)\n",
    "        proposals_labels.extend(labels)\n",
    "        data_list.extend(list(zip(proposal_bbox, labels)))\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "    images = torch.tensor(images)\n",
    "    return data_list, proposals_box_list, resized_images, proposals_labels, images\n",
    "\n",
    "def assign_labels(proposals_bbox, gt_bboxes, gt_label, image, iou_threshold1=0.4, iou_threshold2=0.6): # \n",
    "    labels = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    max_iou_failsafe        = -1    # In case of no positive, use the best \n",
    "    max_iou_failsafe_info   = None  #/\n",
    "    image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    for proposal in proposals_bbox:\n",
    "        proposal_x1, proposal_y1, proposal_w, proposal_h = proposal\n",
    "        proposal_area = proposal_w * proposal_h\n",
    "\n",
    "        max_iou             = -1 # Initialize max IOU score for each proposal\n",
    "        max_label_idx       = -1\n",
    "\n",
    "        for idx, bbox in enumerate(gt_bboxes):\n",
    "            bbox_x1, bbox_y1, bbox_w, bbox_h = bbox\n",
    "\n",
    "            # Compare boxes\n",
    "            intersection_x1 = max(proposal_x1, bbox_x1)\n",
    "            intersection_y1 = max(proposal_y1, bbox_y1)\n",
    "            intersection_x2 = min(proposal_x1 + proposal_w, bbox_x1 + bbox_w)\n",
    "            intersection_y2 = min(proposal_y1 + proposal_h, bbox_y1 + bbox_h)\n",
    "\n",
    "            intersection_area = max(0, intersection_x2 - intersection_x1) * max(0, intersection_y2 - intersection_y1)\n",
    "            bbox_area = bbox_w * bbox_h\n",
    "            union_area = proposal_area + bbox_area - intersection_area\n",
    "\n",
    "            iou = intersection_area / union_area\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "            # Get closest matching box \n",
    "            if iou > max_iou:\n",
    "                max_iou         = iou\n",
    "                max_label_idx   = idx\n",
    "            #\n",
    "            if iou > max_iou_failsafe:\n",
    "                max_iou_failsafe        = iou\n",
    "                max_iou_failsafe_info   = (proposal, idx)\n",
    "            \n",
    "        if   max_iou <= iou_threshold1: labels.append(0)                            # Object proposal is a negative example\n",
    "        elif max_iou >= iou_threshold2: labels.append(int(gt_label[max_label_idx])) # Object proposal is a positive example\n",
    "        else:                           labels.append(-1)                           # Object proposal is not considered\n",
    "    if labels.count(0) == len(labels):\n",
    "        idx         = proposals_bbox.index(max_iou_failsafe_info[0])\n",
    "        labels[idx] = int(gt_label[max_iou_failsafe_info[1]])\n",
    "    # num_ones = len(labels)\n",
    "    # print(f\"Number of objects: {num_ones}\")\n",
    "\n",
    "    # To visualize the bounding boxes and IOU scores, uncomment the following line\n",
    "    #visualize_bboxes(image, proposals, bboxes, iou_scores)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of categories: 18\n",
      "Generating proposals and labels for train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 8/8 [01:03<00:00,  7.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Simple classifier model\n",
      "Start batch 0/3 with 100\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "safe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m     61\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy\n\u001b[0;32m---> 62\u001b[0m train(network, optimizer, \u001b[39m25\u001b[39;49m, loss, train_dl, test_dl, val_dl, \u001b[39mlen\u001b[39;49m(train_ds))\n\u001b[1;32m     65\u001b[0m \u001b[39m# print(\"Training done\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# print(\"Saving model\")\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# if not os.path.exists(\"models\"):\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m#     os.makedirs(\"models\")\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# dump(svm, 'models/svm_model.joblib')\u001b[39;00m\n",
      "File \u001b[0;32m~/myfolder/DLCV_GRP_23/Projects/Project4/src/classifier/train_classifier.py:57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, loss_func, train_dl, test_dl, val_dl, train_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m     train_preds\u001b[39m.\u001b[39mextend(predicted\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     56\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/zhome/a7/d/156506/myfolder/DLCV_GRP_23/trained_models/class_model_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msafe\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Compute the test accuracy\u001b[39;00m\n\u001b[1;32m     59\u001b[0m test_loss \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mException\u001b[0m: safe"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "# ctest = 0\n",
    "# ctrain = 0\n",
    "patch_size = (128,128)\n",
    "batch_size = 100\n",
    "\n",
    "dataset = WasteDatasetImages(transform=transforms.ToTensor(), resize=(512,512))\n",
    "num_classes = dataset.num_categories()\n",
    "print(f\"Total number of categories: {num_classes}\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "num_images_to_process_train = len(train_dataset) #Amount of train images to process\n",
    "num_images_to_process_test = len(test_dataset) #Amount of test images to process\n",
    "max_proposals_per_image = 1000 # Selective search will generate max 1000 proposals per image\n",
    "# # Quick limit for debugging/testing\n",
    "num_images_to_process_train = 8 #Amount of train images to process\n",
    "num_images_to_process_test = 2 #Amount of test images to process\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Run selective search \n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "print(\"Generating proposals and labels for train set\")\n",
    "train_data, train_proposals, train_proposals_image, train_label, images_og = generate_proposals_and_labels(train_dataloader, ss, num_images_to_process_train, max_proposals_per_image, img_shape=patch_size)\n",
    "# plot_images_jupyter(images_og, train_proposals, train_label) # Plot with labels\n",
    "\n",
    "\n",
    "# Extract the features and their corresponding bounding boxes\n",
    "# train_features_flat, train_boxes = zip(*[(feature, bbox) for bbox, feature in train_features])\n",
    "# train_features_flat = list(train_features_flat)\n",
    "# train_boxes = list(train_boxes)\n",
    "\n",
    "# train_labels = [label for _, label in train_data]\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "train_size = int(0.75*len(train_proposals_image))\n",
    "test_size = int(0.15*len(train_proposals_image))\n",
    "val_size = len(train_proposals_image) - train_size - test_size \n",
    "\n",
    "train_ds, test_ds, val_ds = random_split(list(zip(train_proposals_image,train_label)), [train_size, test_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True )\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading Simple classifier model\")\n",
    "network = SimpleClassifier(num_classes, )\n",
    "network.to(device)\n",
    "\n",
    "network = SimpleClassifier(num_classes=num_classes, resolution=patch_size)\n",
    "network.to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "loss = F.cross_entropy\n",
    "train(network, optimizer, 25, loss, train_dl, test_dl, val_dl, len(train_ds))\n",
    "\n",
    "\n",
    "# print(\"Training done\")\n",
    "# print(\"Saving model\")\n",
    "# if not os.path.exists(\"models\"):\n",
    "#     os.makedirs(\"models\")\n",
    "# dump(svm, 'models/svm_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Generating proposals and labels for test set\")\n",
    "# test_data, test_proposals, test_image, test_label = generate_proposals_and_labels(test_dataloader, ss, num_images_to_process_test, max_proposals_per_image)\n",
    "\n",
    "# # Extract the features and their corresponding bounding boxes\n",
    "# test_features_flat, test_boxes = zip(*[(feature, bbox) for bbox, feature in test_features])\n",
    "# test_features_flat = list(test_features_flat)\n",
    "# test_boxes = list(test_boxes)\n",
    "\n",
    "# test_labels = [label for _, label in test_data]\n",
    "# test_labels = np.array(test_labels)\n",
    "\n",
    "# # svm = load('models/svm_model.joblib')\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# # predictions = svm.predict(test_features_flat)\n",
    "\n",
    "# accuracy = accuracy_score(test_labels, predictions)\n",
    "# precision = precision_score(test_labels, predictions)\n",
    "# recall = recall_score(test_labels, predictions)\n",
    "# f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1-score:\", f1)\n",
    "\n",
    "# number_of_images = 20 #Test images to visualize\n",
    "\n",
    "# for i, img in enumerate(test_images[:number_of_images]):\n",
    "#     # features_per_image = test_features_per_image[i]\n",
    "\n",
    "#     predictions_by_image = []\n",
    "#     bbox_list_by_image = []\n",
    "\n",
    "#     # for feature in features_per_image:\n",
    "#     #     bbox, feature_hist = feature \n",
    "#     #     prediction = svm.predict([feature_hist])[0] \n",
    "#     #     predictions_by_image.append(prediction)\n",
    "#     #     bbox_list_by_image.append(bbox)\n",
    "\n",
    "#     object_indices = [j for j, pred in enumerate(predictions_by_image) if pred == 1]            \n",
    "#     object_bboxes = [bbox_list_by_image[index] for index in object_indices]\n",
    "#     draw_bboxes(img, object_bboxes, [1] * len(object_bboxes), i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
