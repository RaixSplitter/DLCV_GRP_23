{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Own\n",
    "from waste_dataset import WasteDatasetImages\n",
    "from classifier.simpleCNN import SimpleClassifier\n",
    "from classifier.train_classifier import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/dtu/datasets1/02514/data_wastedetection\"\n",
    "output_dir = \"output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataloader):    # save image with bboxes\n",
    "    num_images_to_process = 10\n",
    "    for idx, (image, bboxes) in enumerate(dataloader):\n",
    "        if idx >= num_images_to_process:\n",
    "            break\n",
    "\n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox = [item.item() for item in bbox]\n",
    "            x, y, width, height = bbox\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"image_{idx}.jpg\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Processed image {idx + 1}/{len(dataloader)}\")\n",
    "\n",
    "    print(\"Images with bounding boxes saved.\")\n",
    "\n",
    "def plot_images_jupyter(images_og, train_proposals, train_label, num_images = 8):\n",
    "    num_images = num_images\n",
    "    for idx, (image, bboxes, labels) in enumerate(zip(images_og, train_proposals, train_label)):\n",
    "        if idx >= num_images:\n",
    "            break\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        has_label = set()\n",
    "        for bbox,lab in zip(bboxes, labels):\n",
    "            # bbox = [item.item() for item in bbox]\n",
    "            x, y, width, height = bbox\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r' if lab == 0 else \"blue\", facecolor='none', label = lab if lab not in has_label else \"\")\n",
    "            ax.add_patch(rect)\n",
    "            has_label.add(lab)\n",
    "        ax.legend()\n",
    "\n",
    "def draw_bboxes(image, bboxes, labels, index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x, y, width, height = bbox\n",
    "        rect = patches.Rectangle((x,y), width, height, linewidth=1, edgecolor='r' if label == 1 else 'b', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    if not os.path.exists(\"inference\"):\n",
    "        os.makedirs(\"inference\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'inference/(bbox_visualization_on_test_images_{index}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bboxes(image, proposals, ground_truth_bboxes, iou_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for bbox in ground_truth_bboxes:\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "        rect = patches.Rectangle((bbox_x, bbox_y), bbox_w, bbox_h,\n",
    "                                 linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    for proposal, iou_score in zip(proposals, iou_scores):\n",
    "        proposal_x, proposal_y, proposal_w, proposal_h = proposal\n",
    "\n",
    "        if iou_score > 0.2:\n",
    "            rect = patches.Rectangle((proposal_x, proposal_y), proposal_w, proposal_h,\n",
    "                                 linewidth=1, edgecolor='b', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(proposal_x, proposal_y, f\"IOU: {float(iou_score):.2f}\", color='r')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig('bbox_visualization.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_proposals_and_labels(dataloader, ss, num_images_to_process, max_proposals_per_image, img_shape = (224,224)):\n",
    "    data_list           = []\n",
    "    images              = []\n",
    "    resized_images      = []\n",
    "    proposals_box_list  = []\n",
    "    proposals_labels    = []\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for image, gt_bboxes, gt_label in tqdm(dataloader, desc=f\"Processing images\",total=num_images_to_process, leave=True):\n",
    "        if counter >= num_images_to_process:\n",
    "            break\n",
    "        \n",
    "        img = image.squeeze().permute(1, 2, 0).numpy()\n",
    "        images.append(img)\n",
    "        \n",
    "        # selective search\n",
    "        ss.setBaseImage(img)\n",
    "        # ss.switchToSelectiveSearchQuality() #Maybe try with fast\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "        rects = ss.process()\n",
    "\n",
    "        proposal_bbox   = []\n",
    "        proposal_image  = []\n",
    "        labels          = []\n",
    "\n",
    "        for i, rect in enumerate(rects): # loop for each region in image\n",
    "            if i >= max_proposals_per_image:\n",
    "                break\n",
    "            x, y, width, height = rect \n",
    "            bbox = [int(x), int(y), int(width), int(height)]  \n",
    "            proposal_bbox.append(bbox)\n",
    "            \n",
    "            proposal_img = img[y:y+height, x:x+width]\n",
    "            proposal_img = cv2.resize(proposal_img, img_shape)\n",
    "            \n",
    "            # proposal_features = extract_vgg16_features(proposal_image)\n",
    "            proposal_image.append(np.ravel(proposal_img))\n",
    "\n",
    "        labels = assign_labels(proposal_bbox, gt_bboxes, gt_label, image)\n",
    "        \n",
    "        # Remove gray-zones (inbetween threshold)\n",
    "        for i, label in reversed(list(enumerate(labels))):\n",
    "            if label == -1:\n",
    "                del proposal_bbox[i]\n",
    "                del labels[i]\n",
    "        \n",
    "        # Append\n",
    "        proposals_box_list.append(proposal_bbox)\n",
    "        resized_images.append(proposal_image)\n",
    "        proposals_labels.append(labels)\n",
    "        data_list.extend(list(zip(proposal_bbox, labels)))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return data_list, proposals_box_list, resized_images, proposals_labels, images\n",
    "\n",
    "def extract_color_histogram(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist\n",
    "\n",
    "# def extract_vgg16_features(image):\n",
    "#     image = cv2.resize(image, (224, 224))\n",
    "#     image = torch.tensor(image).permute(2, 0, 1).float().to(device)\n",
    "#     image = image.unsqueeze(0)\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         features = vgg16(image)\n",
    "#\n",
    "#     features = features.cpu().numpy() \n",
    "#     features = features.flatten() \n",
    "#\n",
    "#     return features\n",
    "\n",
    "\n",
    "def assign_labels(proposals_bbox, gt_bboxes, gt_label, image, iou_threshold1=0.3, iou_threshold2=0.7): # \n",
    "    labels = []\n",
    "    iou_scores = []\n",
    "\n",
    "    image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    for proposal in proposals_bbox:\n",
    "        proposal_x1, proposal_y1, proposal_w, proposal_h = proposal\n",
    "        proposal_area = proposal_w * proposal_h\n",
    "\n",
    "        max_iou = -1  # Initialize max IOU score for each proposal\n",
    "        max_label_idx = -1\n",
    "\n",
    "        for idx, bbox in enumerate(gt_bboxes):\n",
    "            bbox_x1, bbox_y1, bbox_w, bbox_h = bbox\n",
    "\n",
    "            # Compare boxes\n",
    "            intersection_x1 = max(proposal_x1, bbox_x1)\n",
    "            intersection_y1 = max(proposal_y1, bbox_y1)\n",
    "            intersection_x2 = min(proposal_x1 + proposal_w, bbox_x1 + bbox_w)\n",
    "            intersection_y2 = min(proposal_y1 + proposal_h, bbox_y1 + bbox_h)\n",
    "\n",
    "            intersection_area = max(0, intersection_x2 - intersection_x1) * max(0, intersection_y2 - intersection_y1)\n",
    "            bbox_area = bbox_w * bbox_h\n",
    "            union_area = proposal_area + bbox_area - intersection_area\n",
    "\n",
    "            iou = intersection_area / union_area\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "            # Get closest matching box \n",
    "            if iou > max_iou:\n",
    "                max_iou         = iou\n",
    "                max_label_idx   = idx\n",
    "        if   max_iou <= iou_threshold1: labels.append(0)         # Object proposal is a negative example\n",
    "        elif max_iou >= iou_threshold2: labels.append(int(gt_label[max_label_idx]))  # Object proposal is a positive example\n",
    "        else:                           labels.append(-1)        # Object proposal is not considered\n",
    "    \n",
    "    # num_ones = len(labels)\n",
    "    # print(f\"Number of objects: {num_ones}\")\n",
    "\n",
    "    # To visualize the bounding boxes and IOU scores, uncomment the following line\n",
    "    #visualize_bboxes(image, proposals, bboxes, iou_scores)\n",
    "    return labels    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of categories:18\n",
      "Generating proposals and labels for train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  25%|██▌       | 303/1200 [03:17<09:44,  1.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m ss \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mximgproc\u001b[39m.\u001b[39msegmentation\u001b[39m.\u001b[39mcreateSelectiveSearchSegmentation()\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerating proposals and labels for train set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m train_data, train_proposals, train_proposals_image, train_label, images_og \u001b[39m=\u001b[39m generate_proposals_and_labels(train_dataloader, ss, num_images_to_process_train, max_proposals_per_image, img_shape\u001b[39m=\u001b[39;49mpatch_size)\n\u001b[1;32m     31\u001b[0m \u001b[39m# plot_images_jupyter(images_og, train_proposals, train_label) # Plot with labels\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# train_labels = [label for _, label in train_data]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# train_labels = np.array(train_labels)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.75\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(train_data))\n",
      "Cell \u001b[0;32mIn[70], line 126\u001b[0m, in \u001b[0;36mgenerate_proposals_and_labels\u001b[0;34m(dataloader, ss, num_images_to_process, max_proposals_per_image, img_shape)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# proposal_features = extract_vgg16_features(proposal_image)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     proposal_image\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mravel(proposal_img))\n\u001b[0;32m--> 126\u001b[0m labels \u001b[39m=\u001b[39m assign_labels(proposal_bbox, gt_bboxes, gt_label, image)\n\u001b[1;32m    128\u001b[0m \u001b[39m# Remove gray-zones (inbetween threshold)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39menumerate\u001b[39m(labels))):\n",
      "Cell \u001b[0;32mIn[70], line 189\u001b[0m, in \u001b[0;36massign_labels\u001b[0;34m(proposals_bbox, gt_bboxes, gt_label, image, iou_threshold1, iou_threshold2)\u001b[0m\n\u001b[1;32m    186\u001b[0m intersection_x2 \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(proposal_x1 \u001b[39m+\u001b[39m proposal_w, bbox_x1 \u001b[39m+\u001b[39m bbox_w)\n\u001b[1;32m    187\u001b[0m intersection_y2 \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(proposal_y1 \u001b[39m+\u001b[39m proposal_h, bbox_y1 \u001b[39m+\u001b[39m bbox_h)\n\u001b[0;32m--> 189\u001b[0m intersection_area \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39m0\u001b[39;49m, intersection_x2 \u001b[39m-\u001b[39;49m intersection_x1) \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, intersection_y2 \u001b[39m-\u001b[39m intersection_y1)\n\u001b[1;32m    190\u001b[0m bbox_area \u001b[39m=\u001b[39m bbox_w \u001b[39m*\u001b[39m bbox_h\n\u001b[1;32m    191\u001b[0m union_area \u001b[39m=\u001b[39m proposal_area \u001b[39m+\u001b[39m bbox_area \u001b[39m-\u001b[39m intersection_area\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "ctest = 0\n",
    "ctrain = 0\n",
    "patch_size  = (64,64)\n",
    "\n",
    "dataset = WasteDatasetImages(transform=transforms.ToTensor(), resize=(224, 224))\n",
    "num_classes = dataset.num_categories()\n",
    "print(f\"Total number of categories: {num_classes}\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "num_images_to_process_train = len(train_dataset) #Amount of train images to process\n",
    "num_images_to_process_test = len(test_dataset) #Amount of test images to process\n",
    "max_proposals_per_image = 1000 # Selective search will generate max 1000 proposals per image\n",
    "# # Quick limit for debugging/testing\n",
    "# num_images_to_process_train = 8 #Amount of train images to process\n",
    "# num_images_to_process_test = 2 #Amount of test images to process\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Run selective search \n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "print(\"Generating proposals and labels for train set\")\n",
    "train_data, train_proposals, train_proposals_image, train_label, images_og = generate_proposals_and_labels(train_dataloader, ss, num_images_to_process_train, max_proposals_per_image, img_shape=patch_size)\n",
    "# plot_images_jupyter(images_og, train_proposals, train_label) # Plot with labels\n",
    "\n",
    "\n",
    "# Extract the features and their corresponding bounding boxes\n",
    "# train_features_flat, train_boxes = zip(*[(feature, bbox) for bbox, feature in train_features])\n",
    "# train_features_flat = list(train_features_flat)\n",
    "# train_boxes = list(train_boxes)\n",
    "\n",
    "# train_labels = [label for _, label in train_data]\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "train_size = int(0.75*len(train_data))\n",
    "test_size = int(0.15*len(train_data))\n",
    "val_size = len(train_data) - train_size - test_size \n",
    "\n",
    "train_ds, test_ds, val_ds = random_split(train_data, [train_size, test_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "val_dl = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading Simple classifier model\")\n",
    "network = SimpleClassifier(num_classes, )\n",
    "network.to(device)\n",
    "\n",
    "\n",
    "\n",
    "network = SimpleClassifier(num_classes=num_classes, resolution=patch_size)\n",
    "network.to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "loss = F.cross_entropy\n",
    "train(network, optimizer, 25, loss, train_dl, test_dl, val_dl, len(train_ds))\n",
    "\n",
    "\n",
    "print(\"Training done\")\n",
    "print(\"Saving model\")\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# dump(svm, 'models/svm_model.joblib')\n",
    "\n",
    "\n",
    "print(\"Generating proposals and labels for test set\")\n",
    "test_data, test_proposals, test_image, test_label = generate_proposals_and_labels(test_dataloader, ss, num_images_to_process_test, max_proposals_per_image)\n",
    "\n",
    "# Extract the features and their corresponding bounding boxes\n",
    "test_features_flat, test_boxes = zip(*[(feature, bbox) for bbox, feature in test_features])\n",
    "test_features_flat = list(test_features_flat)\n",
    "test_boxes = list(test_boxes)\n",
    "\n",
    "test_labels = [label for _, label in test_data]\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# svm = load('models/svm_model.joblib')\n",
    "\n",
    "# Make predictions on the test data\n",
    "# predictions = svm.predict(test_features_flat)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "number_of_images = 20 #Test images to visualize\n",
    "\n",
    "for i, img in enumerate(test_images[:number_of_images]):\n",
    "    # features_per_image = test_features_per_image[i]\n",
    "\n",
    "    predictions_by_image = []\n",
    "    bbox_list_by_image = []\n",
    "\n",
    "    # for feature in features_per_image:\n",
    "    #     bbox, feature_hist = feature \n",
    "    #     prediction = svm.predict([feature_hist])[0] \n",
    "    #     predictions_by_image.append(prediction)\n",
    "    #     bbox_list_by_image.append(bbox)\n",
    "\n",
    "    object_indices = [j for j, pred in enumerate(predictions_by_image) if pred == 1]            \n",
    "    object_bboxes = [bbox_list_by_image[index] for index in object_indices]\n",
    "    draw_bboxes(img, object_bboxes, [1] * len(object_bboxes), i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
